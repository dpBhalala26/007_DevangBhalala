# -*- coding: utf-8 -*-
"""0_logistic_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C5S1dB07Az-KCEeUOXojxr3v3Z4qzASm
"""

#Importing libraries
import numpy as np 
import pandas as pd 
import io
import matplotlib.pyplot as plt

# reading the csv file, del 2 columns from the file, checking first few rows of the file
from google.colab import files
uploaded = files.upload()

data = pd.read_csv(io.BytesIO(uploaded['BuyComputer.csv']))

data.drop(columns=['User ID',],axis=1,inplace=True)
data.head()

#Declare label as last column in the source file
y = data.iloc[:,-1].values

#Declaring X as all columns excluding last
X = data.iloc[:,:-1].values

# Splitting data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 7)

# Sacaling data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Variabes to calculate sigmoid function
y_pred = []
len_x = len(X_train[0])
w = []
b = 0.2
print(len_x)

entries = len(X_train[:,0])
entries

for weights in range(len_x):
    w.append(0)
w

# Sigmoid function
def sigmoid(z):
    return (1/(1+np.exp(-z)))

def predict(inputs):
    z = np.dot(w,inputs)+b
    a = sigmoid(z)
    return a

#Loss function
def loss_func(y,a):
    J = 0
    return J

dw = []
db = 0
J = 0
alpha = 0.1
for x in range(len_x):
    dw.append(0)

#Repeating the process 3000 times
for itr in range(3000):
    for i in range(entries):
        local_x = X_train[i]
        a = predict(local_x)   
        dz = a - y_train[i]
        J += loss_func(y_train[i],a)
        for j in range(len_x):
            dw[j] = dw[j]+(local_x[j]*dz)
        db += dz
    J = J/entries
    db = db/entries
    for x in range(len_x):
        dw[x]=dw[x]/entries
    for x in range(len_x):
        w[x] = w[x]-(alpha*dw[x])
    b = b-(alpha*db)         
    J=0

#Print weight
w

#print bias
b

#predicting the label
for x in range(len(y_test)):
    y_pred.append(predict(X_test[x]))

#print actual and predicted values in a table
for x in range(len(y_pred)):
    print('Actual value is: ', y_test[x], '\tPredicted value is: ', y_pred[x])
    if y_pred[x]>=0.5:
        y_pred[x]=1
    else:
        y_pred[x]=0

# Calculating accuracy of prediction
count = 0
for x in range(len(y_pred)):
    if(y_pred[x]==y_test[x]):
        count=count+1
print('Accuracy is: ',(count/(len(y_pred)))*100)

"""#Using sklearn LogisticRegression model"""

# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(random_state = 7)

#Fit
LR.fit(X_train, y_train)

#predicting the test label with LR. Predict always takes X as input
y_predLR = LR.predict(X_test)

y_predLR

"""**Exercise:**

Try logistic regression on BuyComputer dataset and set Random state=Your_RollNumber (last 3 digit of ID, incase if you don't have ID)
"""